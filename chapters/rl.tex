\فصل{یادگیری تقویتی}

یادگیری ماشینی یکی از زیرشاخه‌های اصلی و کاربردی در حوزه هوش مصنوعی و علوم کامپیوتر می‌باشد. این علم به جستجو و یافتن الگوریتم‌هایی به منظور ایجاد قابلیت یادگیری در عامل‌ها می‌پردازد. به عبارت دیگر هدف یادگیری ماشینی این است که یک سامانه بدون دریافت صریح دستورات، بتواند داده‌های ورودی را به صورت اتوماتیک تحلیل و بر اساس تحلیل‌های به‌دست آمده، در مورد داده‌های جدید
تصمیم‌گیری نماید.

\قسمت{تعریف یادگیری تقویتی}

یادگیری تقویتی یک روش یادگیری بدون ناظر می‌باشد و هدف آن یادگیری یک سیاست و نگاشتی از مشاهدات به اعمال، بر مبنای دریافت بازخورد ازمحیط میباشد. این عمل یادگیری را میتوان به صورت جستجوی مجموعهای از سیاستها بهصورت لحظهای که در تعامل با محیط ارزیابی میشوند بیان کرد.

در یک مسئله یادگیری تقویتی با عاملی مواجه هستیم که از طریق سعی و خطا با محیط تعامل کرده و یاد می‌گیرد تا عملی بهینه را برای رسیدن به هدف مورد نظر سیستم انتخاب کند. در این نوع یادگیری هیچ ناظر خارجی وجود ندارد و عامل به تنهایی و به طور مستقل با محیط تعامل کرده، یاد می‌گیرد، تجربه کسب
می‌کند و پاداشی را متناسب با عمل خود دریافت می‌کند.

ایده اصلی روش‌های یادگیری تقویتی از روی الگوی یادگیری رفتار انسان اقتباس شده است. در این
روش، هیچ راهنما و ناظری به صورت مستقیم بر چگونگی آموزش سیستم تمرکز ندارد. الگوریتم یادگیری با بررسی میزان مطلوبیت نتیجه حاصل از یک عمل و به واسطه اهدای پاداش یا جریمه به تمامی اجزایی که در
حصول این نتیجه نقش داشته‌اند، سعی در اعمال آموزش به این سیستم را دارد.

\قسمت{مدل یادگیری تقویتی استاندارد}

در یک مدل یادگیری تقویتی استاندارد، یک عامل از طریق ادراکات و اعمال خود با محیط در ارتباط
است. مطابق شکل ~\رجوع{شکل:مدل یادگیری تقویتی}، در هر مرحله عامل وضعیت جاری $s$ را به عنوان ورودی دریافت می‌کند و عمل $a$ را برای تولید خروجی انتخاب می‌کند.

\شروع{شکل}
[t]\centerfig{rl-robot-brain.tex}{1.2}
\شرح{یک مدل یادگیری تقویتی استاندارد \cite{Kaelbling1996}}
\برچسب{شکل:مدل یادگیری تقویتی}
\پایان{شکل}

به ازای هر عمل، عامل ارزش عمل خود را به عنوان یک سیگنال پاداش
$r$ به همراه وضعیت جدید سیستم $i$ را دریافت میکند. پاداش دریافتی می‌تواند مثبت یا منفی باشد. سیستم  کنترل اعمال عامل می‌بایست اعمالی را انتخاب کند که مجموع سیگنال‌های پاداش را افزایش دهد. یک عامل
می‌تواند انجام این سیستم کنترلی را در فرآیند تعامل آزمون و خطای خود با محیط یاد بگیرد.

اجزای تشکیل‌دهنده‌ی این مدل شامل:

\شروع{فقرات}
\فقره یک مجموعه‌ی گسسته از حالت‌های محیط $S$،
\فقره یک مجموعه‌ی گسسته از اعمال $A$،
\فقره یک مجموعه از سیگنال‌های پاداش عددی، به طور معمول بین ۰ و ۱ یا اعداد حقیقی.
\پایان{فقرات}

تابع $R$ تعیین‌کننده‌ی پاداش دریافتی عامل از انجام عمل $a$ در حالت $s$ و تابع $T$ تعیین‌کننده‌ی حالت
بعدی که عامل در اثر این زوج حالت/عمل به آن منتقل می‌شود، می‌باشند. و نیز تابع $I$ تعیین‌کننده‌ی نحوه‌ی مشاهده‌ی حالات محیط توسط عامل می‌باشد. فرض بر این است که عامل
دقیقا همان چیزی را می‌بیند که حالت محیط است.

\قسمت{اجزاء یادگیری تقویتی}
اجزاء اصلی تشکیل‌دهنده‌ی یک سیستم یادگیری تقویتی را می‌توان به چهار بخش زیر تقسیم نمود:
\شروع{فقرات}
\فقره سیاست\LTRfootnote{Policy}:
یک سیاست، نگاشتی از فضای حالت به فضای عمل است و تعیین می‌کند که عامل در
مواجهه با حالات مختلف، باید چه عملی را انجام دهد. به عبارت دیگر سیاست یک عامل، نحوه 
رفتار وی را مشخص می‌کند. منظور از فضای حالت، مجموعه‌ای از وضعیت‌های دریافت شده از محیط می‌باشد. فضای عمل، مجموعه‌ای از اعمال قابل انجام در آن وضعیت است. پیروی از یک
سیاست خوب، عامل را به نتیج‌های بهتر و کارآمدتر می‌رساند.
\فقره تابع پاداش\LTRfootnote{Reward Function}:
این تابع، میزان مطلوبیت هر وضعیت یا زوج وضعیت/عمل را با تخصیص یک مقدار
عددی به عنوان پاداش (که می‌تواند مثبت یا منفی باشد)، مشخص می‌کند. این تابع به وسیله این  مقدار تعیین می‌کند که کدام عمل برای عامل خوب و کدام عمل بد است. هدف یک عامل، انتخاب
سلسله مراتبی از اعمال است که میزان پاداش کل دریافتی از سیستم را بیشینه کند.
\فقره تابع ارزش\LTRfootnote{Value Function}:
این تابع مطلوبیت بلندمدت را تعیین می‌کند. ارزش وضعیتی که عامل در آن قرار دارد
برابر با امید حاصل‌جمع پاداش‌هایی است که عامل در وضعیت فعلی و تمامی وضعیت‌های دنبال‌شده
 در آینده، دریافت می‌کند. تفاوت تابع پاداش و ارزش در این است که پاداش، سود دریافتی آنی از انجام یک عمل است، اما ارزش، یک اثر بلندمدت دارد. گاهی اوقات ممکن است یک عمل پاداش
کمی داشته باشد اما در بلند مدت سبب افزایش ارزش کلی سیستم شود و بالعکس.
\فقره مدل محیط:
این مدل می‌تواند رفتار یک سیستم را تقلید کند. به عبارت دیگر از روی این مدل،
می‌توان با داشتن وضعیت فعلی عامل و عملی که انجام می‌دهد، پاداش دریافتی از محیط و وضعیت  بعدی که عامل در آن قرار خواهد گرفت را پیش‌بینی کرد. این مدل برای طرح‌ریزی کردن عامل استفاده می‌شود و کمک می‌کند تا عامل، قبل از انجام عمل در محیط به صورت واقعی، پیامدهای
انجام آن را بررسی کند.
\پایان{فقرات}

\قسمت{فرآیندهای تصمیم‌گیری مارکف}
مسائل یادگیری تقویتی اغلب به خوبی توسط فرآیندهای تصمیم‌گیری مارکف\LTRfootnote{Markov Decision Process (MDP)} مدل می‌شوند.
فرآیند تصمیم‌گیری مارکف، مدلی برای تصمیم‌گیری‌های ترتیبی می‌باشد و در مسائلی که خروجی‌ها به صورت غیرقطعی می‌باشند مورد استفاده قرار می‌گیرد. منظور از تصمیم‌گیری‌های ترتیبی این است که کارایی عامل به مجموعه‌ای از تصمیمات که به صورت متوالی اتخاذ گردیده بستگی دارد و تنها وابسته به تصمیم فعلی
عامل نخواهد بود.

یک مدل MDP معمولا به صورت چندتایی <\LRE{$S, A, T, R$}> تعریف می‌شود. این مدل از اجزای زیر
تشکیل شده است \cite{Kaelbling1996}:

\شروع{فقرات}

\فقره  یک مجموعه از حالات $S$
\فقره  یک مجموعه اعمال $A$
\فقره یک تابع پاداش ${\Bbb R}: S \times A \rightarrow {\Bbb R}$
\فقره یک تابع انتقال حالت $T: S \times A \rightarrow \prod(S)$، که در آن $\prod(S)$ مجموعه‌ای از توزیع‌های تصادفی بر
روی مجموعه‌ی $S$ است. \LRE{$T(s, a, s^{\prime})$} به عنوان احتمال انتقال از حالت $s$ به حالت $s^{\prime}$ با اتخاذ عمل $a$ درنظر گرفته می‌شود.

\پایان{فقرات}

در فرآیند تصمیم‌گیری مارکف معین، عامل می‌تواند مجموعه حالات متمایز محیط را درک کند. همچنین می‌تواند از مجموعه اعمال موجود در مجموعه $A$ عملی را انتخاب و آن را در محیط اجرا کند. محیط با دادن پاداش حاصل از اجرای عمل و نیز انتقال عامل به وضعیت بعدی (طبق تابع انتقال)، به عامل پاسخ می‌دهد.

به نسبت دیگر الگوریتم‌های ارائه شده برای یادگیری تقویتی، الگوریتم یادگیری-$Q$ از کاربرد بیشتری برخوردار می‌باشد. با به کارگیری این دسته از الگوریتم‌ها می‌توان یک راهکار مسیریابی خودمختار به صورت وفقی\LTRfootnote{Adapted} بدست آورد \cite{Nedzelnitsky1987}. با این حال، این الگوریتم یک روش متمرکز است که برای
یادگیری یک سیستم تک‌عامله به کار می‌رود.

%\قسمت{يادگیری تقويتی عمیق}


